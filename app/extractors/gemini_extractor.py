# app/extractors/gemini_extractor.py
import os
import json
import logging
import time
from typing import Dict, Any, List, Optional, Callable
import re 
import math
import numpy as np
from PIL import Image 

from app.config import GEMINI_API_KEY, GEMINI_MODEL, CONVERTED_DIR
from app.extractors.base import BaseExtractor
from app.extractors.context_agent import ContextAgent
from app.extractors.extraction_agent import ExtractionAgent
from app.extractors.color_mapping_agent import ColorMappingAgent
from app.extractors.layout_detetion_agent import LayoutDetetionAgent
from app.extractors.generic_strategy_agent import GenericStrategyAgent
from app.extractors.validators.size_color_validation_agent import SizeColorValidationAgent, SizeColorValidationResult
from app.extractors.validators.validation_agent import ValidationAgent, ValidationResult

from app.utils.file_utils import convert_pdf_to_images
from app.utils.barcode_generator import add_barcodes_to_extraction_result, add_barcodes_to_products
from app.data.reference_data import (get_supplier_code, get_markup, get_category,SUPPLIER_MAP, COLOR_MAP, SIZE_MAP,CATEGORIES)
from app.utils.json_utils import safe_json_dump, fix_nan_in_products, sanitize_for_json
from app.utils.supplier_assignment import determine_best_supplier, assign_supplier_to_products

logger = logging.getLogger(__name__)

try:
    HAS_VALIDATION = True
    logger.info("‚úÖ Sistema de valida√ß√£o melhorado carregado")
except ImportError:
    HAS_VALIDATION = False
    logger.warning("‚ö†Ô∏è Sistema de valida√ß√£o n√£o dispon√≠vel")

try:
    HAS_SIZE_VALIDATION = True
    logger.info("‚úÖ Sistema de valida√ß√£o de tamanhos carregado")
except ImportError:
    HAS_SIZE_VALIDATION = False
    logger.warning("‚ö†Ô∏è Sistema de valida√ß√£o de tamanhos n√£o dispon√≠vel")

try:
    from app.utils.json_utils import safe_json_dump, fix_nan_in_products, sanitize_for_json
    has_json_utils = True
except ImportError:
    has_json_utils = False
    logger.warning("M√≥dulo json_utils n√£o encontrado, usar serializa√ß√£o padr√£o")

class GeminiExtractor(BaseExtractor):
    def __init__(self, api_key: str = GEMINI_API_KEY):
        self.api_key = api_key
        self.context_agent = ContextAgent(api_key)
        self.extraction_agent = ExtractionAgent(api_key)
        self.ai_color_mapping_agent = ColorMappingAgent()
        
        self.layout_detector = LayoutDetetionAgent(api_key)
        self.strategy_agent = GenericStrategyAgent()

        self.validation_agent = ValidationAgent(api_key)
        
        if HAS_SIZE_VALIDATION:
            try:
                self.size_validator = SizeColorValidationAgent(api_key)
                logger.info("‚úÖ Validador de tamanhos e cores inicializado")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao inicializar validador de tamanhos: {e}")
                self.size_validator = None
            else:
                self.size_validator = None

        self.current_layout_analysis = {}
        self.current_strategy = None
        self.page_results_history = []

    async def analyze_context(self, document_path: str) -> str:
        logger.info("üîß Usando an√°lise cl√°ssica")
        
        context_info = await self.context_agent.analyze_document(document_path)
        
        logger.info("Detectando layout do documento...")
        layout_analysis = await self.layout_detector.analyze_document_structure(document_path)
        
        layout_type = layout_analysis.get('layout_type', 'UNKNOWN')
        confidence = layout_analysis.get('confidence', 0.0)
        logger.info(f"Layout detectado: {layout_type} (confian√ßa: {confidence:.2f})")
        
        logger.info("Selecionando estrat√©gia de extra√ß√£o...")
        strategy = self.strategy_agent.select_strategy(
            layout_analysis=layout_analysis,
            page_number=1
        )
        logger.info(f"Estrat√©gia selecionada: {strategy.name}")
        
        self.current_context_info = context_info
        self.current_layout_analysis = layout_analysis
        self.current_strategy = strategy
        
        enhanced_context = self._enhance_context_with_layout_and_strategy(
            context_info, layout_analysis, strategy
        )
        
        return enhanced_context

    async def _retry_extraction_with_different_strategy(self, 
                                                   document_path: str,
                                                   recommendations: List[str]) -> List[Dict]:
        try:
            logger.info("üîÑ Tentando re-extra√ß√£o com estrat√©gia alternativa...")
            
            # Analisar recomenda√ß√µes para escolher estrat√©gia
            retry_focus = []
            
            if any("tamanho" in rec.lower() for rec in recommendations):
                retry_focus.append("sizes")
            if any("quantidade" in rec.lower() for rec in recommendations):
                retry_focus.append("quantities")
            if any("agrup" in rec.lower() for rec in recommendations):
                retry_focus.append("grouping")
            
            # Executar re-extra√ß√£o focada
            if "sizes" in retry_focus or "quantities" in retry_focus:
                return await self._focused_size_quantity_extraction(document_path)
            elif "grouping" in retry_focus:
                return await self._focused_grouping_extraction(document_path)
            else:
                return await self._generic_retry_extraction(document_path)
                
        except Exception as e:
            logger.warning(f"Erro na re-extra√ß√£o: {e}")
            return []

    async def _focused_size_quantity_extraction(self, document_path: str) -> List[Dict]:
        """
        Re-extra√ß√£o focada em tamanhos e quantidades corretos
        """
        try:
            images = self._get_document_images_safe(document_path)
            if not images:
                return []
            
            context = self.current_context_info or {}
            
            # Prompt especializado em tamanhos e quantidades
            focused_prompt = f"""
            RE-EXTRA√á√ÉO FOCADA EM TAMANHOS E QUANTIDADES
            
            PROBLEMA DETECTADO: Tamanhos incompletos ou quantidades incorretas
            
            INSTRU√á√ïES ESPEC√çFICAS:
            1. Para cada produto, leia TODOS os tamanhos da linha (XS, S, M, L, XL, XXL)
            2. Leia as quantidades EXATAS de cada tamanho (incluindo 0)
            3. N√ÉO assuma quantidade 1 para todos
            4. Verifique alinhamento posicional entre colunas
            
            FORMATO DE RESPOSTA:
            {{
                "products": [
                    {{
                        "product_name": "...",
                        "material_code": "...",
                        "colors": [
                            {{
                                "color_code": "...",
                                "color_name": "...",
                                "unit_price": 0.0,
                                "sizes": [
                                    {{"size": "XS", "quantity": 0}},
                                    {{"size": "S", "quantity": 1}},
                                    {{"size": "M", "quantity": 2}},
                                    {{"size": "L", "quantity": 1}},
                                    {{"size": "XL", "quantity": 0}}
                                ]
                            }}
                        ]
                    }}
                ]
            }}
            
            CR√çTICO: Incluir TODOS os tamanhos vis√≠veis, mesmo com quantidade 0
            """
            
            result = await self.extraction_agent.extract_from_page(
                images[0], focused_prompt, 1, len(images), []
            )
            
            return result.get('products', [])
            
        except Exception as e:
            logger.warning(f"Erro na re-extra√ß√£o focada: {e}")
            return []

    async def _focused_grouping_extraction(self, document_path: str) -> List[Dict]:
        """
        Re-extra√ß√£o focada em agrupamento correto de produtos
        """
        try:
            images = self._get_document_images_safe(document_path)
            if not images:
                return []
            
            grouping_prompt = f"""
            RE-EXTRA√á√ÉO FOCADA EM AGRUPAMENTO DE PRODUTOS
            
            PROBLEMA DETECTADO: Produtos duplicados por cor
            
            INSTRU√á√ïES:
            1. Identifique produtos com c√≥digos similares (ex: CF5271MA96E.1, CF5271MA96E.2)
            2. Agrupe-os em UM produto com m√∫ltiplas cores
            3. N√ÉO crie produtos separados para cada cor
            
            EXEMPLO CORRETO:
            {{
                "products": [
                    {{
                        "material_code": "CF5271MA96E",
                        "product_name": "Malha Gola Redonda",
                        "colors": [
                            {{
                                "color_code": "M9799",
                                "color_name": "Nero",
                                "sizes": [...tamanhos para esta cor...]
                            }},
                            {{
                                "color_code": "012",
                                "color_name": "Bege", 
                                "sizes": [...tamanhos para esta cor...]
                            }}
                        ]
                    }}
                ]
            }}
            """
            
            result = await self.extraction_agent.extract_from_page(
                images[0], grouping_prompt, 1, len(images), []
            )
            
            return result.get('products', [])
            
        except Exception as e:
            logger.warning(f"Erro na re-extra√ß√£o de agrupamento: {e}")
            return []

    async def _generic_retry_extraction(self, document_path: str) -> List[Dict]:
        """
        Re-extra√ß√£o gen√©rica com prompt mais rigoroso
        """
        try:
            images = self._get_document_images_safe(document_path)
            if not images:
                return []
            
            context = self.current_context_info.copy() if self.current_context_info else {}
            context.update({
                'extraction_mode': 'retry_conservative',
                'focus': 'accuracy_over_speed',
                'validation': 'strict'
            })
            
            result = await self.extraction_agent.extract_from_page(
                images[0], context, 1, len(images), []
            )
            
            return result.get('products', [])
            
        except Exception as e:
            logger.warning(f"Erro no retry gen√©rico: {e}")
            return []
    
    async def extract_with_validation(self, document_path: str, 
                                enable_validation: bool = True,
                                max_retries: int = 1) -> Dict[str, Any]:

        logger.info(f"üîç Iniciando extra√ß√£o com valida√ß√£o MELHORADA para: {os.path.basename(document_path)}")
        
        try:
            # 1. Extra√ß√£o inicial (m√©todo existente)
            logger.info("üöÄ Executando extra√ß√£o inicial...")
            extraction_result = await self.extract(document_path)
            initial_products = extraction_result.get('products', [])
            
            logger.info(f"üìä Extra√ß√£o inicial: {len(initial_products)} produtos")
            
            # 2. Se valida√ß√£o desabilitada ou n√£o dispon√≠vel
            if not enable_validation or not HAS_VALIDATION or not self.validation_agent:
                logger.info("üìÑ Executando sem valida√ß√£o")
                extraction_result['validation'] = {
                    'enabled': False,
                    'message': 'Valida√ß√£o desabilitada ou n√£o dispon√≠vel'
                }
                return extraction_result
            
            # 3. Detectar p√°ginas que falharam (m√©todo existente)
            failed_pages = []
            if hasattr(self, 'page_results_history'):
                for i, page_result in enumerate(self.page_results_history):
                    if page_result.get('error') or page_result.get('products_count', 0) == 0:
                        failed_pages.append(i + 1)
            
            if failed_pages:
                logger.info(f"üîç P√°ginas com falha detectadas: {failed_pages}")
            
            # 4. Obter imagens para valida√ß√£o
            images = self._get_document_images_safe(document_path)
            if not images:
                logger.warning("‚ö†Ô∏è Sem imagens para valida√ß√£o visual")
                extraction_result['validation'] = {
                    'enabled': False,
                    'message': 'Sem imagens para an√°lise visual'
                }
                return extraction_result
            
            logger.info(f"üñºÔ∏è Carregadas {len(images)} imagens para valida√ß√£o")
            
            # 5. Executar valida√ß√£o MELHORADA
            logger.info("üîç Executando valida√ß√£o com corre√ß√µes espec√≠ficas...")
            
            context = {
                'document_type': extraction_result.get('document_type', ''),
                'supplier': extraction_result.get('supplier', ''),
                'brand': extraction_result.get('brand', ''),
                'file_name': os.path.basename(document_path)
            }
            
            validation_result = await self.validation_agent.validate_extraction(
                extracted_products=initial_products,
                original_context=context,
                pdf_pages=images,
                layout_analysis=self.current_layout_analysis
            )
            
            # 6. Log detalhado das corre√ß√µes
            logger.info(f"üìà Valida√ß√£o conclu√≠da:")
            logger.info(f"   - Produtos iniciais: {len(initial_products)}")
            logger.info(f"   - Produtos finais: {len(validation_result.products)}")
            logger.info(f"   - Produtos agrupados: {validation_result.products_merged}")
            logger.info(f"   - Tamanhos corrigidos: {validation_result.sizes_corrected}")
            logger.info(f"   - Quantidades corrigidas: {validation_result.quantities_corrected}")
            logger.info(f"   - Confian√ßa: {validation_result.confidence_score:.2f}")
            
            # 7. Log das corre√ß√µes aplicadas
            if validation_result.corrections_applied:
                logger.info("üîß Corre√ß√µes aplicadas:")
                for correction in validation_result.corrections_applied[:5]:
                    logger.info(f"   - {correction}")
                if len(validation_result.corrections_applied) > 5:
                    logger.info(f"   - ... e mais {len(validation_result.corrections_applied) - 5}")
            
            # 8. Log das recomenda√ß√µes
            if validation_result.recommendations:
                logger.info("üí° Recomenda√ß√µes:")
                for rec in validation_result.recommendations:
                    logger.info(f"   - {rec}")
            
            # 9. Retry se confian√ßa muito baixa
            retry_count = 0
            while (validation_result.confidence_score < 0.5 and 
                retry_count < max_retries):
                
                retry_count += 1
                logger.warning(f"Confian√ßa baixa ({validation_result.confidence_score:.2f}) - Retry {retry_count}")
                
                # Re-extra√ß√£o com estrat√©gia diferente (m√©todo existente)
                alternative_products = await self._retry_extraction_with_different_strategy(
                    document_path, validation_result.recommendations
                )
                
                if alternative_products:
                    validation_result = await self.validation_agent.validate_extraction(
                        extracted_products=alternative_products,
                        original_context=context,
                        pdf_pages=images,
                        layout_analysis=self.current_layout_analysis
                    )
            
            # 10. Preparar resultado final melhorado
            enhanced_result = extraction_result.copy()
            enhanced_result.update({
                'products': validation_result.products,
                'validation': {
                    'enabled': True,
                    'confidence_score': validation_result.confidence_score,
                    'completeness_score': validation_result.completeness_score,
                    'consistency_score': validation_result.consistency_score,
                    'visual_completeness_score': validation_result.visual_completeness_score,
                    'density_score': validation_result.density_score,
                    'extraction_method': validation_result.extraction_method,
                    'validation_errors': validation_result.validation_errors,
                    'missing_fields': validation_result.missing_fields,
                    'recommendations': validation_result.recommendations,
                    'total_pages_processed': validation_result.total_pages_processed,
                    'products_initial': len(initial_products),
                    'products_final': len(validation_result.products),
                    'products_merged': validation_result.products_merged,
                    'sizes_corrected': validation_result.sizes_corrected,
                    'quantities_corrected': validation_result.quantities_corrected,
                    'corrections_applied': validation_result.corrections_applied,
                    'failed_pages_detected': failed_pages
                }
            })
            
            # 11. Status final
            if validation_result.confidence_score >= 0.8:
                logger.info(f"‚úÖ EXCELENTE: Alta confian√ßa ({validation_result.confidence_score:.2f})")
            elif validation_result.confidence_score >= 0.6:
                logger.info(f"‚ö†Ô∏è BOM: Confian√ßa m√©dia ({validation_result.confidence_score:.2f})")
            else:
                logger.warning(f"‚ùå ATEN√á√ÉO: Baixa confian√ßa ({validation_result.confidence_score:.2f})")
            
            return enhanced_result
            
        except Exception as e:
            logger.exception(f"‚ùå Erro na valida√ß√£o: {str(e)}")
            
            # Fallback para resultado original
            extraction_result['validation'] = {
                'enabled': False,
                'error': str(e),
                'message': 'Valida√ß√£o falhou - resultado da extra√ß√£o normal'
            }
            return extraction_result
    
    def _analyze_improvements(self, 
                        initial_products: List[Dict], 
                        corrected_products: List[Dict]) -> List[str]:
        """
        Analisa e reporta melhorias espec√≠ficas feitas
        """
        improvements = []
        
        # Detectar produtos agrupados
        if len(corrected_products) < len(initial_products):
            merged_count = len(initial_products) - len(corrected_products)
            improvements.append(f"Agrupados {merged_count} produtos duplicados por cor")
        
        # Detectar corre√ß√µes de tamanhos
        initial_sizes = set()
        corrected_sizes = set()
        
        for product in initial_products:
            for color in product.get('colors', []):
                for size in color.get('sizes', []):
                    initial_sizes.add(size.get('size', ''))
        
        for product in corrected_products:
            for color in product.get('colors', []):
                for size in color.get('sizes', []):
                    corrected_sizes.add(size.get('size', ''))
        
        # Verificar se houve mudan√ßas significativas nos tamanhos
        size_changes = corrected_sizes - initial_sizes
        if size_changes:
            improvements.append(f"Corrigidos tamanhos incorretos: {', '.join(list(size_changes)[:3])}")
        
        # Detectar melhorias na completude
        initial_complete = sum(1 for p in initial_products if self._is_product_complete(p))
        corrected_complete = sum(1 for p in corrected_products if self._is_product_complete(p))
        
        if corrected_complete > initial_complete:
            improvement = corrected_complete - initial_complete
            improvements.append(f"Recuperados dados completos para {improvement} produtos")
        
        return improvements

    def _is_product_complete(self, product: Dict) -> bool:
        """Verifica se produto tem dados completos"""
        if not product.get('material_code') or not product.get('product_name'):
            return False
        
        colors = product.get('colors', [])
        if not colors:
            return False
        
        for color in colors:
            sizes = color.get('sizes', [])
            if sizes and any(s.get('quantity', 0) > 0 for s in sizes):
                return True
        
        return False

    def _get_document_images_safe(self, document_path: str) -> List[Image.Image]:
        """M√©todo melhorado para obter imagens"""
        try:
            if not document_path.lower().endswith('.pdf'):
                logger.info("Documento n√£o √© PDF - valida√ß√£o visual limitada")
                return []
            
            # Usar m√©todo existente de convers√£o
            image_paths = convert_pdf_to_images(document_path, CONVERTED_DIR)
            
            if not image_paths:
                logger.warning("N√£o foi poss√≠vel converter PDF para imagens")
                return []
            
            images = []
            for img_path in image_paths:
                try:
                    img = Image.open(img_path)
                    images.append(img)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao carregar imagem {img_path}: {e}")
            
            logger.info(f"üñºÔ∏è Carregadas {len(images)} imagens para valida√ß√£o")
            return images
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao obter imagens do documento: {e}")
            return []
        
    async def extract_with_size_validation(self, document_path: str, confidence_threshold: float = 0.7) -> Dict[str, Any]:
     
        logger.info(f"üìè Iniciando extra√ß√£o com valida√ß√£o de tamanhos para: {os.path.basename(document_path)}")
        
        try:
            # 1. Executar extra√ß√£o normal primeiro
            logger.info("üöÄ Executando extra√ß√£o inicial...")
            extraction_result = await self.extract(document_path)
            initial_products = extraction_result.get('products', [])
            
            logger.info(f"üìä Extra√ß√£o inicial: {len(initial_products)} produtos")
            
            # 2. Se n√£o h√° valida√ß√£o dispon√≠vel, retornar resultado normal
            if not HAS_SIZE_VALIDATION or not self.size_validator:
                logger.info("üìÑ Valida√ß√£o de tamanhos n√£o dispon√≠vel")
                extraction_result['size_validation'] = {
                    'enabled': False,
                    'message': 'Sistema n√£o dispon√≠vel'
                }
                return extraction_result
            
            # 3. Obter imagens para an√°lise visual
            images = self._get_document_images_safe(document_path)
            if not images:
                logger.warning("‚ö†Ô∏è Sem imagens para valida√ß√£o visual de tamanhos")
                extraction_result['size_validation'] = {
                    'enabled': False,
                    'message': 'Sem imagens para an√°lise'
                }
                return extraction_result
            
            logger.info(f"üñºÔ∏è Carregadas {len(images)} imagens para valida√ß√£o")
            
            # 4. Executar valida√ß√£o de tamanhos e cores
            logger.info("üîç Executando valida√ß√£o de tamanhos e cores...")
            
            validation_result = await self.size_validator.validate_and_correct(
                products=initial_products,
                page_images=images,
                confidence_threshold=confidence_threshold
            )
            
            # 5. Analisar resultados da valida√ß√£o
            corrected_products = validation_result.corrected_products
            corrections_made = validation_result.corrections_made
            
            logger.info(f"üìà Valida√ß√£o de tamanhos conclu√≠da:")
            logger.info(f"   - Produtos iniciais: {len(initial_products)}")
            logger.info(f"   - Produtos finais: {len(corrected_products)}")
            logger.info(f"   - Corre√ß√µes aplicadas: {len(corrections_made)}")
            logger.info(f"   - Confian√ßa: {validation_result.confidence_score:.2f}")
            
            # 6. Log das corre√ß√µes aplicadas
            if corrections_made:
                logger.info("üîß Corre√ß√µes aplicadas:")
                for correction in corrections_made[:5]:  # Primeiras 5
                    logger.info(f"   - {correction}")
                if len(corrections_made) > 5:
                    logger.info(f"   - ... e mais {len(corrections_made) - 5} corre√ß√µes")
            
            # 7. Detectar melhorias espec√≠ficas
            improvements = self._analyze_improvements(initial_products, corrected_products)
            if improvements:
                logger.info("‚ú® Melhorias detectadas:")
                for improvement in improvements:
                    logger.info(f"   - {improvement}")
            
            # 8. Preparar resultado final
            enhanced_result = extraction_result.copy()
            enhanced_result.update({
                'products': corrected_products,
                'size_validation': {
                    'enabled': True,
                    'confidence_score': validation_result.confidence_score,
                    'corrections_made': corrections_made,
                    'validation_errors': validation_result.validation_errors,
                    'size_alignment_issues': validation_result.size_alignment_issues,
                    'color_grouping_issues': validation_result.color_grouping_issues,
                    'products_initial': len(initial_products),
                    'products_final': len(corrected_products),
                    'products_merged': len(initial_products) - len(corrected_products),
                    'improvements_detected': improvements,
                    'confidence_threshold_used': confidence_threshold
                }
            })
            
            # 9. Status final
            if validation_result.confidence_score >= 0.8:
                logger.info(f"‚úÖ Alta confian√ßa nas corre√ß√µes ({validation_result.confidence_score:.2f})")
            elif validation_result.confidence_score >= 0.6:
                logger.info(f"‚ö†Ô∏è Confian√ßa m√©dia ({validation_result.confidence_score:.2f}) - verificar resultados")
            else:
                logger.warning(f"‚ùå Baixa confian√ßa ({validation_result.confidence_score:.2f}) - poss√≠veis problemas")
            
            return enhanced_result
            
        except Exception as e:
            logger.exception(f"‚ùå Erro na valida√ß√£o de tamanhos: {str(e)}")
            
            # Fallback para resultado original
            extraction_result['size_validation'] = {
                'enabled': False,
                'error': str(e),
                'message': 'Valida√ß√£o falhou - resultado da extra√ß√£o normal'
            }
            return extraction_result

    async def _alternative_structure_extraction(self, document_path: str) -> List[Dict]:
        """Extra√ß√£o com abordagem estrutural alternativa"""
        try:
            # For√ßar re-an√°lise de layout com estrat√©gia diferente
            alternative_layout = await self.layout_detector.analyze_document_structure(document_path)
            
            # Selecionar estrat√©gia diferente
            alternative_strategy = self.strategy_agent.select_strategy(
                layout_analysis=alternative_layout,
                page_number=1,
                previous_results=self.page_results_history
            )
            
            if alternative_strategy.name != self.current_strategy.name:
                logger.info(f"Usando estrat√©gia alternativa: {alternative_strategy.name}")
                
                # Re-extrair com nova estrat√©gia
                images = self._get_document_images(document_path)
                if images:
                    enhanced_context = self._enhance_context_with_layout_and_strategy(
                        self.current_context_info, alternative_layout, alternative_strategy
                    )
                    
                    result = await self.extraction_agent.extract_from_page(
                        images[0], enhanced_context, 1, len(images), []
                    )
                    
                    return result.get('products', [])
            
            return []
            
        except Exception as e:
            logger.warning(f"Erro na extra√ß√£o estrutural alternativa: {e}")
            return []

    async def _generic_retry_extraction(self, document_path: str) -> List[Dict]:
        """Retry gen√©rico com par√¢metros ajustados"""
        try:
            # Extrair novamente mas com prompt modificado para ser mais rigoroso
            images = self._get_document_images(document_path)
            if not images:
                return []
            
            # Criar contexto mais espec√≠fico para retry
            retry_context = self.current_context_info.copy() if self.current_context_info else {}
            retry_context.update({
                'extraction_mode': 'retry',
                'focus': 'complete_products_only',
                'strictness': 'high'
            })
            
            result = await self.extraction_agent.extract_from_page(
                images[0], retry_context, 1, len(images), []
            )
            
            return result.get('products', [])
            
        except Exception as e:
            logger.warning(f"Erro no retry gen√©rico: {e}")
            return []
        
    def _enhance_context_with_layout_and_strategy(
        self, 
        context_info: Dict[str, Any], 
        layout_analysis: Dict[str, Any], 
        strategy: Any
    ) -> str:

        original_context = self.context_agent.format_context_for_extraction(context_info)
        
        layout_info = [
            "\n## LAYOUT DETECTADO AUTOMATICAMENTE",
            f"**Tipo**: {layout_analysis.get('layout_type', 'UNKNOWN')}",
            f"**Confian√ßa**: {layout_analysis.get('confidence', 0):.2f}",
            f"**Estrat√©gia**: {layout_analysis.get('extraction_strategy', 'adaptive')}"
        ]
        
        technical = layout_analysis.get("technical_analysis", {})
        if not technical.get("error"):
            column_info = technical.get("column_detection", {})
            if column_info.get("column_count", 0) > 0:
                layout_info.append(f"**Colunas detectadas**: {column_info['column_count']}")
        
        strategy_info = [
            f"\n## ESTRAT√âGIA SELECIONADA: {strategy.name.upper()}",
            f"**Abordagem**: {strategy.approach}"
        ]
        
        # Instru√ß√µes espec√≠ficas da estrat√©gia
        strategy_info.append("\n### INSTRU√á√ïES ESPEC√çFICAS:")
        for key, instruction in strategy.specific_instructions.items():
            readable_key = key.replace('_', ' ').title()
            strategy_info.append(f"- **{readable_key}**: {instruction}")
        
        # Instru√ß√µes do layout detectado
        extraction_instructions = layout_analysis.get("extraction_instructions", {})
        if extraction_instructions:
            strategy_info.append("\n### INSTRU√á√ïES BASEADAS NO LAYOUT:")
            for key, instruction in extraction_instructions.items():
                if instruction and key != "special_considerations":
                    readable_key = key.replace('_', ' ').title()
                    strategy_info.append(f"- **{readable_key}**: {instruction}")
        
        # Combinar tudo
        return "\n".join([
            original_context,
            "\n".join(layout_info),
            "\n".join(strategy_info),
            "\n## REGRAS CR√çTICAS:",
            "- Seguir rigorosamente a estrat√©gia selecionada",
            "- Adaptar se a estrutura mudar entre p√°ginas",
            "- Extrair apenas dados claramente vis√≠veis"
        ])

    async def process_page(
        self, 
        image_path: str, 
        context: str,
        page_number: int,
        total_pages: int,
        previous_result: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:

        logger.info(f"Processando p√°gina {page_number}/{total_pages}")
        
        if page_number > 1 and self.page_results_history:
            last_result = self.page_results_history[-1]
            new_strategy = self.strategy_agent.adapt_strategy_for_page(
                self.current_strategy, 
                last_result, 
                page_number - 1,
                self.current_layout_analysis
            )
            
            if new_strategy:
                logger.info(f"ADAPTA√á√ÉO: {self.current_strategy.name} ‚Üí {new_strategy.name}")
                self.current_strategy = new_strategy
                
                # Atualizar contexto com nova estrat√©gia
                context = self._update_context_with_new_strategy(context, new_strategy, page_number)
        
        # Processar p√°gina (usa o ExtractionAgent original)
        page_result = await self.extraction_agent.process_page(
            image_path, context, page_number, total_pages, previous_result
        )
        
        # NOVA: Armazenar resultado para adapta√ß√£o
        page_result["_strategy_used"] = self.current_strategy.name if self.current_strategy else "unknown"
        self.page_results_history.append(page_result)
        
        # Log dos resultados
        products_found = len(page_result.get("products", []))
        has_error = "error" in page_result
        strategy_name = self.current_strategy.name if self.current_strategy else "unknown"
        
        logger.info(f"P√°gina {page_number}: {products_found} produtos extra√≠dos "
                   f"(estrat√©gia: {strategy_name}){' COM ERRO' if has_error else ''}")
        
        return page_result
    
    def _update_context_with_new_strategy(
        self, 
        context: str, 
        new_strategy: Any, 
        page_number: int
    ) -> str:

        strategy_update = f"""
            ## ESTRAT√âGIA ADAPTADA PARA P√ÅGINA {page_number}

            ‚ö†Ô∏è **MUDAN√áA DE ESTRAT√âGIA**
            - Nova estrat√©gia: {new_strategy.name}
            - Motivo: Resultado anterior insatisfat√≥rio
            - Aplicar novas instru√ß√µes:

            ### INSTRU√á√ïES ATUALIZADAS:
            """
                    
        for key, instruction in new_strategy.specific_instructions.items():
            readable_key = key.replace('_', ' ').title()
            strategy_update += f"- **{readable_key}**: {instruction}\n"
        
        return context + strategy_update

    async def extract_document(
        self, 
        document_path: str,
        job_id: str,
        jobs_store: Dict[str, Any],
        update_progress_callback: Callable
    ) -> Dict[str, Any]:

        start_time = time.time()
        
        try:
            logger.info(f"üöÄ INICIANDO EXTRA√á√ÉO - Job: {job_id}")  # ADICIONAR
            
            jobs_store[job_id]["model_results"]["gemini"] = {
                "model_name": GEMINI_MODEL,
                "status": "processing",
                "progress": 5.0
            }
            
            # ETAPA 1: An√°lise melhorada (contexto + layout + estrat√©gia)
            is_pdf = document_path.lower().endswith('.pdf')
            
            if is_pdf:
                logger.info(f"üìÑ Processando PDF: {document_path}")  # ADICIONAR
                
                jobs_store[job_id]["model_results"]["gemini"]["progress"] = 10.0
                logger.info(f"=== AN√ÅLISE GEN√âRICA INICIADA ===")
                logger.info(f"Documento: {os.path.basename(document_path)}")
                
                # NOVA: An√°lise completa (contexto + layout + estrat√©gia)
                logger.info("üîç Iniciando an√°lise de contexto...")  # ADICIONAR
                context_description = await self.analyze_context(document_path)
                logger.info("‚úÖ An√°lise de contexto conclu√≠da")  # ADICIONAR
                
                context_info = self.current_context_info
                
                logger.info(f"An√°lise completa conclu√≠da:")
                logger.info(f"- Layout: {self.current_layout_analysis.get('layout_type', 'UNKNOWN')}")
                logger.info(f"- Estrat√©gia: {self.current_strategy.name if self.current_strategy else 'N/A'}")
            else:
                logger.info("üìÑ Documento n√£o √© PDF, usando configura√ß√£o b√°sica")  # ADICIONAR
                context_description = "Documento de pedido ou nota de encomenda"
                context_info = {"document_type": "Documento de pedido", "supplier": "", "brand": ""}
            
            # ETAPA 2-4: Mant√©m-se igual (preparar imagens + extra√ß√£o + cores)
            logger.info("üñºÔ∏è Preparando imagens...")  # ADICIONAR
            jobs_store[job_id]["model_results"]["gemini"]["progress"] = 15.0
            
            if is_pdf:
                logger.info("üì∏ Convertendo PDF para imagens...")  # ADICIONAR
                image_paths = convert_pdf_to_images(document_path, CONVERTED_DIR)
                logger.info(f"‚úÖ {len(image_paths)} imagens criadas")  # ADICIONAR
            else:
                image_paths = [document_path]
                
            total_pages = len(image_paths)
            logger.info(f"üìã Preparadas {total_pages} imagens para processamento")
            
            # Extra√ß√£o com adapta√ß√£o autom√°tica
            logger.info("ü§ñ Iniciando extra√ß√£o com IA...")  # ADICIONAR
            combined_result = {"products": [], "order_info": {}}
            
            if context_info:
                combined_result["order_info"] = {
                    "supplier": context_info.get("supplier", ""),
                    "document_type": context_info.get("document_type", ""),
                    "order_number": context_info.get("reference_number", ""),
                    "date": context_info.get("date", ""),
                    "customer": context_info.get("customer", ""),
                    "brand": context_info.get("brand", ""),
                    "season": context_info.get("season", "")
                }
            
            progress_per_page = 80.0 / total_pages
            
            for page_num, img_path in enumerate(image_paths, start=1):
                logger.info(f"üìÑ Processando p√°gina {page_num}/{total_pages}: {os.path.basename(img_path)}")  # ADICIONAR
                
                current_progress = 15.0 + (page_num - 1) * progress_per_page
                jobs_store[job_id]["model_results"]["gemini"]["progress"] = current_progress
                
                # NOVA: Processamento com adapta√ß√£o autom√°tica
                logger.info(f"üîç Enviando p√°gina {page_num} para an√°lise IA...")  # ADICIONAR
                page_result = await self.process_page(
                    img_path,
                    context_description,
                    page_num,
                    total_pages,
                    combined_result if page_num > 1 else None
                )
                logger.info(f"‚úÖ P√°gina {page_num} processada")  # ADICIONAR
                
                # Verificar erro (mant√©m-se igual)
                if "error" in page_result and not page_result.get("products"):
                    logger.error(f"‚ùå Erro ao processar p√°gina {page_num}: {page_result['error']}")
                    if page_num == 1:
                        raise ValueError(f"Falha ao processar a primeira p√°gina: {page_result['error']}")
                    continue
                
                # Mesclar resultados (mant√©m-se igual)
                if "products" in page_result:
                    products_found = len(page_result.get("products", []))
                    logger.info(f"üì¶ P√°gina {page_num}: {products_found} produtos encontrados")  # ADICIONAR
                    combined_result["products"].extend(page_result.get("products", []))
                
                if "order_info" in page_result and page_result["order_info"]:
                    for key, value in page_result["order_info"].items():
                        if value and (key not in combined_result["order_info"] or not combined_result["order_info"].get(key)):
                            combined_result["order_info"][key] = value
                
                jobs_store[job_id]["model_results"]["gemini"]["progress"] = 15.0 + page_num * progress_per_page
            
            total_products = len(combined_result["products"])
            logger.info(f"üéâ EXTRA√á√ÉO CONCLU√çDA - Total de produtos: {total_products}")
            
            if combined_result["products"]:
                try:
                    mapped_products = self.ai_color_mapping_agent.map_product_colors(
                        combined_result["products"]
                    )
                    combined_result["products"] = mapped_products
                    
                    mapping_report = self.ai_color_mapping_agent.get_mapping_report()
                    combined_result["_ai_color_mapping"] = mapping_report
                    
                    stats = mapping_report['statistics']
                    if stats['mappings_details']:
                        for change in stats['mappings_details'][:3]:
                            confidence = change.get('confidence', 'unknown')
                            logger.info(f"  '{change['original_name']}' ({change['original_code']}) ‚Üí '{change['mapped_name']}' ({change['mapped_code']}) [confidence: {confidence}]")
                
                except Exception as e:
                    logger.error(f"Erro no mapeamento AI de cores: {str(e)}")
                    combined_result["_ai_color_mapping"] = {"error": str(e)}
            
            # P√≥s-processamento (mant√©m-se igual)
            processed_products, determined_supplier = self._post_process_products(combined_result["products"], context_info)
            combined_result["order_info"]["supplier"] = determined_supplier
            
            if has_json_utils:
                supplier = context_info.get("supplier", "")
                supplier_code = get_supplier_code(supplier) if supplier else None
                markup = 2.73
                
                if supplier_code:
                    markup_value = get_markup(supplier_code)
                    if markup_value:
                        markup = markup_value
                
                processed_products = fix_nan_in_products(processed_products, markup=markup)
                logger.info("Produtos sanitizados para evitar valores NaN no JSON")

            combined_result["products"] = processed_products
            logger.info(f"P√≥s-processamento: {len(combined_result['products'])} produtos √∫nicos identificados")
            
            processing_time = time.time() - start_time
            
            # NOVA: Metadados melhorados
            strategy_adaptations = len(set(r.get("_strategy_used", "") for r in self.page_results_history)) - 1
            
            combined_result["_metadata"] = {
                "pages_processed": total_pages,
                "context_description": context_description,
                "processing_approach": "enhanced_with_generic_system",
                "processing_time_seconds": processing_time,
                "context_info": context_info,
                "layout_analysis": self.current_layout_analysis,
                "final_strategy": self.current_strategy.name if self.current_strategy else "unknown",
                "strategy_adaptations": strategy_adaptations,
                "agents_used": ["ContextAgent", "LayoutDetectionAgent", "GenericStrategyAgent", "ExtractionAgent", "ColorMappingAgent"],
            }
            
            # Log final melhorado
            logger.info(f"=== EXTRA√á√ÉO CONCLU√çDA ===")
            logger.info(f"Produtos extra√≠dos: {len(combined_result['products'])}")
            logger.info(f"Tempo total: {processing_time:.2f}s")
            logger.info(f"Layout detectado: {self.current_layout_analysis.get('layout_type', 'UNKNOWN')}")
            logger.info(f"Estrat√©gia final: {self.current_strategy.name if self.current_strategy else 'N/A'}")
            logger.info(f"Adapta√ß√µes: {strategy_adaptations}")
            
            # Atualizar job (mant√©m-se igual)
            jobs_store[job_id]["model_results"]["gemini"] = {
                "model_name": GEMINI_MODEL,
                "status": "completed",
                "progress": 100.0,
                "result": combined_result,
                "processing_time": processing_time
            }
            
            results_file = os.path.join(os.path.dirname(CONVERTED_DIR), "results", f"{job_id}_gemini.json")
            if has_json_utils:
                success = safe_json_dump(combined_result, results_file)
                if success:
                    logger.info(f"Resultado salvo com sucesso em: {results_file}")
                else:
                    logger.error(f"Falha ao salvar resultado em: {results_file}")
            else:
                try:
                    def sanitize_basic(obj):
                        import math
                        if isinstance(obj, dict):
                            return {k: sanitize_basic(v) for k, v in obj.items()}
                        elif isinstance(obj, list):
                            return [sanitize_basic(item) for item in obj]
                        elif isinstance(obj, float) and (math.isnan(obj) or math.isinf(obj)):
                            return 0.0
                        else:
                            return obj
                    
                    sanitized_result = sanitize_basic(combined_result)
                    
                    with open(results_file, "w") as f:
                        json.dump(sanitized_result, f, indent=2)
                    logger.info(f"Resultado salvo com sanitiza√ß√£o b√°sica em: {results_file}")
                except Exception as e:
                    logger.error(f"Erro ao salvar resultado: {str(e)}")
            
            update_progress_callback(job_id)
            
            rocessing_time = time.time() - start_time
            logger.info(f"‚è±Ô∏è Tempo total de processamento: {processing_time:.2f}s")  
            logger.info(f"üìä Taxa de produtos por segundo: {total_products/processing_time:.2f}")

            return combined_result
                
        except Exception as e:
            error_message = f"Erro durante o processamento: {str(e)}"
            
            jobs_store[job_id]["model_results"]["gemini"] = {
                "model_name": GEMINI_MODEL,
                "status": "failed",
                "progress": 0.0,
                "error": error_message,
                "processing_time": time.time() - start_time
            }
            
            update_progress_callback(job_id)
            
            return {"error": error_message}
    
    def _post_process_products(self, products: List[Dict[str, Any]], context_info: Dict[str, Any]) -> List[Dict[str, Any]]:
        
        processed_products = []
        seen_material_codes = set()
        ref_counters = {}
        
        # ETAPA 1: DETERMINAR FORNECEDOR DO DOCUMENTO (APENAS UMA VEZ)
        supplier_name, supplier_code, markup = determine_best_supplier(context_info)
        original_brand = context_info.get("brand", "")

        # Log do resumo da determina√ß√£o
        logger.info(f"Fornecedor determinado: '{supplier_name}' (c√≥digo: {supplier_code}, markup: {markup})")
        
        # ETAPA 2: PROCESSAR PRODUTOS (SEM L√ìGICA DE FORNECEDOR INDIVIDUAL)
        for product in products:
            # Verificar se produto tem c√≥digo de material
            material_code = product.get("material_code")
            if not material_code:
                logger.warning(f"Produto sem c√≥digo de material ignorado: {product.get('name', 'sem nome')}")
                continue
            
            # Limpeza do nome do produto
            product_name = product.get("name", "")
            if product_name is None: 
                product_name = ""
            pattern = r'^([A-Za-z\s]+)(?:\s+\d+.*)?$'
            match = re.match(pattern, product_name)
            
            if match:
                clean_name = match.group(1).strip()
                product["name"] = clean_name
            else:
                clean_name = re.sub(r'\d+', '', product_name).strip()
                clean_name = re.sub(r'\s+', ' ', clean_name).strip()
                product["name"] = clean_name
                
            # Verificar se tem cores v√°lidas
            has_valid_colors = False
            if "colors" in product and isinstance(product["colors"], list):
                for color in product["colors"]:
                    if "sizes" in color and isinstance(color["sizes"], list) and len(color["sizes"]) > 0:
                        has_valid_colors = True
                        break
            
            # Se for produto v√°lido, verificar duplica√ß√£o
            if has_valid_colors:
                # NORMALIZA√á√ÉO DE CATEGORIA
                original_category = product.get("category", "")
                category_upper = original_category.upper() if original_category else ""
                
                # Garantir categoria consistente
                if any(term in category_upper for term in ['POLO', 'POLOSHIRT']):
                    normalized_category = "POLOS"
                elif any(term in category_upper for term in ['SWEATER', 'SWEAT', 'MALHA', 'JERSEY']):
                    normalized_category = "MALHAS"
                else:
                    # Para outras categorias, procurar correspond√™ncia em CATEGORIES
                    normalized_category = None
                    for category in CATEGORIES:
                        if category in category_upper or category_upper in category:
                            normalized_category = category
                            break
                    
                    # Se n√£o encontrar, usar "ACESS√ìRIOS" como fallback
                    if not normalized_category:
                        normalized_category = "ACESS√ìRIOS"
                
                # Atualizar a categoria do produto
                product["category"] = normalized_category
                
                # Logging para debug
                if original_category != normalized_category:
                    logger.info(f"Categoria normalizada: '{original_category}' ‚Üí '{normalized_category}' para produto '{product['name']}'")
                
                # Verificar se j√° processamos este produto (pelo c√≥digo de material)
                if material_code in seen_material_codes:
                    
                    # Mesclar com produto existente
                    for existing_product in processed_products:
                        if existing_product.get("material_code") == material_code:
                            # Mesclar cores n√£o duplicadas
                            existing_color_codes = {c.get("color_code") for c in existing_product.get("colors", [])}
                            
                            for color in product.get("colors", []):
                                color_code = color.get("color_code")
                                if color_code and color_code not in existing_color_codes:
                                    # Adicionar cor ainda n√£o existente
                                    existing_product["colors"].append(color)
                                    existing_color_codes.add(color_code)
                            
                            # Recalcular total_price
                            subtotals = [color.get("subtotal", 0) for color in existing_product["colors"] 
                                        if color.get("subtotal") is not None]
                            existing_product["total_price"] = sum(subtotals) if subtotals else None
                            
                            break
                else:
                    # Novo produto, adicionar √† lista de processados
                    seen_material_codes.add(material_code)
                    
                    # Inicializar contador para este c√≥digo de material
                    if material_code not in ref_counters:
                        ref_counters[material_code] = 0
                    
                    # Adicionar campo de refer√™ncias para cada cor e tamanho
                    product_references = []
                    
                    for color in product.get("colors", []):
                        color_code = color.get("color_code", "")
                        color_name = color.get("color_name", "")
                        
                        for size_info in color.get("sizes", []):
                            size = size_info.get("size", "")
                            quantity = size_info.get("quantity", 0)
                            
                            if quantity <= 0:
                                continue
                            
                            # Incrementar contador para este material
                            ref_counters[material_code] += 1
                            counter = ref_counters[material_code]
                            
                            # Criar refer√™ncia completa
                            reference = f"{material_code}.{counter}"
                            
                            # Criar descri√ß√£o formatada
                            description = f"{product['name']}[{color_code}/{size}]"
                            
                            # Adicionar refer√™ncia √† lista
                            product_references.append({
                                "reference": reference,
                                "counter": counter,
                                "color_code": color_code,
                                "color_name": color_name,
                                "size": size,
                                "quantity": quantity,
                                "description": description
                            })
                    
                    product["references"] = product_references
                    processed_products.append(product)
        
        # ETAPA 3: ATRIBUIR FORNECEDOR A TODOS OS PRODUTOS (APENAS UMA VEZ)
        processed_products = assign_supplier_to_products(processed_products, supplier_name, markup)
        
        # ETAPA 3.5: GARANTIR QUE TODOS OS CAMPOS EST√ÉO CORRETOS
        for product in processed_products:
            # Preservar marca original se existir
            if original_brand and original_brand not in ["", "Marca n√£o identificada"]:
                product["brand"] = original_brand
            
            # For√ßar o fornecedor normalizado
            product["supplier"] = supplier_name
            
            # Garantir que cores t√™m fornecedor correto
            for color in product.get("colors", []):
                color["supplier"] = supplier_name
            
            # CR√çTICO: Garantir que refer√™ncias t√™m fornecedor correto
            for reference in product.get("references", []):
                reference["supplier"] = supplier_name

        # ETAPA 4: FINALIZAR
        processed_products.sort(key=lambda p: p.get("material_code", ""))
        
        try:
            from app.utils.barcode_generator import add_barcodes_to_products
            processed_products = add_barcodes_to_products(processed_products)
        except ImportError:
            logger.warning("M√≥dulo barcode_generator n√£o encontrado, pulando gera√ß√£o de c√≥digos de barras")
        
        # RETORNAR OS DADOS PARA ATUALIZAR O ORDER_INFO NO M√âTODO PRINCIPAL
        return processed_products, supplier_name